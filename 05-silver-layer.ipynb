{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b301f41f-d8fa-4cff-87b9-ae5340e9554e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab3af27-62d7-4b32-b69d-604bb7062786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view_name):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name \n",
    "        \n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view_name)\n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9daa38-e6b5-4565-a567-b07c976db07c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CDCUpserter:\n",
    "    def __init__(self, merge_query, temp_view_name, id_column, sort_by):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name \n",
    "        self.id_column = id_column\n",
    "        self.sort_by = sort_by \n",
    "        \n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.sql import functions as F\n",
    "        \n",
    "        window = Window.partitionBy(self.id_column).orderBy(F.col(self.sort_by).desc())\n",
    "        \n",
    "        df_micro_batch.filter(F.col(\"update_type\").isin([\"new\", \"update\"])) \\\n",
    "                .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\") \\\n",
    "                .createOrReplaceTempView(self.temp_view_name)\n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8acd22-446b-416b-8555-59e0cf90a037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class silver:\n",
    "    def __init__(self,env):\n",
    "        self.config = Config()\n",
    "        self.catalog = env\n",
    "        self.db_name =self.config.db_name\n",
    "        self.checkpoint_base = self.Conf.base_dir_checkpoint + \"/checkpoints\"\n",
    "        self.maxFilesPerTrigger = self.config.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "    def upsert_users (self, once = True, processing_time = '5 seconds',startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        query =(f\"\"\"\n",
    "                Merge into {self.catalog}.{self.db_name}.users a\n",
    "                using users_delta b\n",
    "                where a.user_id = b.user_id\n",
    "                when not matched insert *\n",
    "                \"\"\")\n",
    "        data_upserter=Upserter(query, \"users_delta\")\n",
    "        \n",
    "        df_delta = (\n",
    "                    spark.readStream.option('startingVersion', {startingVersion})\n",
    "                    .option('ignoreDeletes', True)\n",
    "                    .table(f\"{self.catalog}.{self.db_name}.registered_users_bz\")\n",
    "                    .selectExpr(\"user_id\", \"device_id\", \"mac_address\", \"cast(registration_timestamp as timestamp)\")\n",
    "                    .withWatermark(\"registration_timestamp\", \"30 seconds\")\n",
    "                    .dropDuplicates([\"user_id\", \"device_id\"])\n",
    "                    )\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode('update')\n",
    "            .option('checkpointLocation',f'{self.checkpoint_base}/users')\n",
    "            .queryName('users_upsert_stream')\n",
    "        )\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\",'silver_p2')\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def upsert_gym_logs(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "                  merge into {self.catalog}.{self.db_name}.gym_logs a \n",
    "                  using gym_logs_delta b\n",
    "                  ON a.mac_address=b.mac_address AND a.gym=b.gym AND a.login=b.login\n",
    "                  when matched and b.logout > a.login and b.logout > a.logout\n",
    "                  then update set a.logout = b.logout\n",
    "                  when not matched then insert * \n",
    "                  \"\"\"\n",
    "        default_upserter = Upserter(query, \"gym_logs_delta\")\n",
    "        df_delta = (\n",
    "                spark.readStream.option('startingVersion', {startingVersion})\n",
    "                .option('ignoreDeletes',True)\n",
    "                .table(f\"{self.catalog}.{self.db_name}.gym_logins_bz\")\n",
    "                .selectExpr(\"mac_address\",\"gym\",\"cast(login as timestamp)\",\"cast(logout as timestamp)\")\n",
    "                .withWatermark(\"login\",\"30 seconds\")\n",
    "                .dropDuplicates([\"mac_address\",\"gym\",\"login\"])\n",
    "            )\n",
    "\n",
    "        stream_writer =(\n",
    "                df_delta.writeStream.forEachBatch(default_upserter.upsert)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_base}/gym_logs\")\n",
    "                .queryName(\"gym_logs_upsert_stream\")\n",
    "            )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p2\")\n",
    "        if once:\n",
    "                return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "                return stream_writer.trigger(processingTime=processing_time).start()\n",
    "    \n",
    "    \n",
    "    def upsert_user_profile(self, once=False, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        #Idempotent - Insert new record\n",
    "        #           - Ignore deletes\n",
    "        #           - Update user details when\n",
    "        #               1. update_type in (\"new\", \"append\")\n",
    "        #               2. current update is newer than the earlier\n",
    "        schema = \"\"\"\n",
    "            user_id bigint, update_type STRING, timestamp FLOAT, \n",
    "            dob STRING, sex STRING, gender STRING, first_name STRING, last_name STRING, \n",
    "            address STRUCT<street_address: STRING, city: STRING, state: STRING, zip: INT>\n",
    "            \"\"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.user_profile a\n",
    "            USING user_profile_cdc b\n",
    "            ON a.user_id=b.user_id\n",
    "            WHEN MATCHED AND a.updated < b.updated\n",
    "              THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED\n",
    "              THEN INSERT *\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter = CDCUpserter(query, \"user_profile_cdc\", \"user_id\", \"updated\")\n",
    "        \n",
    "        df_cdc = (spark.readStream\n",
    "                       .option(\"startingVersion\", startingVersion)\n",
    "                       .option(\"ignoreDeletes\", True)\n",
    "                       #.option(\"withEventTimeOrder\", \"true\")\n",
    "                       #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                       .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "                       .filter(\"topic = 'user_info'\")\n",
    "                       .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                       .select(\"v.*\")\n",
    "                       .select(\"user_id\", F.to_date('dob','MM/dd/yyyy').alias('dob'),\n",
    "                               'sex', 'gender','first_name','last_name', 'address.*',\n",
    "                               F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n",
    "                               \"update_type\")\n",
    "                       .withWatermark(\"updated\", \"30 seconds\")\n",
    "                       .dropDuplicates([\"user_id\", \"updated\"])\n",
    "                 )\n",
    "    \n",
    "        stream_writer = (df_cdc.writeStream\n",
    "                               .foreachBatch(data_upserter.upsert) \n",
    "                               .outputMode(\"update\") \n",
    "                               .option(\"checkpointLocation\", f\"{self.checkpoint_base}/user_profile\") \n",
    "                               .queryName(\"user_profile_stream\")\n",
    "                        )\n",
    "        \n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    \n",
    "    def upsert_workouts(self, once=False, processing_time=\"10 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F        \n",
    "        schema = \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\"\n",
    "        \n",
    "        #Idempotent - User cannot have two workout sessions at the same time. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.workouts a\n",
    "            USING workouts_delta b\n",
    "            ON a.user_id=b.user_id AND a.time=b.time\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\"\n",
    "\n",
    "        data_upserter=Upserter(query, \"workouts_delta\")\n",
    "        \n",
    "        df_delta = (spark.readStream\n",
    "                         .option(\"startingVersion\", startingVersion)\n",
    "                         .option(\"ignoreDeletes\", True)\n",
    "                         #.option(\"withEventTimeOrder\", \"true\")\n",
    "                         #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                         .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "                         .filter(\"topic = 'workout'\")\n",
    "                         .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                         .select(\"v.*\")\n",
    "                         .select(\"user_id\", \"workout_id\", \n",
    "                                 F.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"), \n",
    "                                 \"action\", \"session_id\")\n",
    "                         .withWatermark(\"time\", \"30 seconds\")\n",
    "                         .dropDuplicates([\"user_id\", \"time\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\",f\"{self.checkpoint_base}/workouts\")\n",
    "                                 .queryName(\"workouts_upsert_stream\")\n",
    "                        )\n",
    "        \n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "        \n",
    "    def upsert_heart_rate(self, once=False, processing_time=\"10 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        \n",
    "        schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "        \n",
    "        #Idempotent - Only one BPM signal is allowed at a timestamp. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "        MERGE INTO {self.catalog}.{self.db_name}.heart_rate a\n",
    "        USING heart_rate_delta b\n",
    "        ON a.device_id=b.device_id AND a.time=b.time\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"heart_rate_delta\")\n",
    "    \n",
    "        df_delta = (spark.readStream\n",
    "                         .option(\"startingVersion\", startingVersion)\n",
    "                         .option(\"ignoreDeletes\", True)\n",
    "                         #.option(\"withEventTimeOrder\", \"true\")\n",
    "                         #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                         .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "                         .filter(\"topic = 'bpm'\")\n",
    "                         .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                         .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, False).otherwise(True).alias(\"valid\"))\n",
    "                         .withWatermark(\"time\", \"30 seconds\")\n",
    "                         .dropDuplicates([\"device_id\", \"time\"])\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/heart_rate\")\n",
    "                                 .queryName(\"heart_rate_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p2\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "          \n",
    "    \n",
    "    def age_bins(self, dob_col):\n",
    "        from pyspark.sql import functions as F\n",
    "        age_col = F.floor(F.months_between(F.current_date(), dob_col)/12).alias(\"age\")\n",
    "        return (F.when((age_col < 18), \"under 18\")\n",
    "                .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "                .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "                .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "                .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "                .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "                .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "                .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "                .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "                .when((age_col >= 95), \"95+\")\n",
    "                .otherwise(\"invalid age\").alias(\"age\"))\n",
    "        \n",
    "    \n",
    "    def upsert_user_bins(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        \n",
    "        # Idempotent - This table is maintained as SCD Type 1 dimension\n",
    "        #            - Insert new user_id records \n",
    "        #            - Update old records using the user_id\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.user_bins a\n",
    "            USING user_bins_delta b\n",
    "            ON a.user_id=b.user_id\n",
    "            WHEN MATCHED \n",
    "              THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"user_bins_delta\")\n",
    "        \n",
    "        df_user = spark.table(f\"{self.catalog}.{self.db_name}.users\").select(\"user_id\")\n",
    "        \n",
    "        # Running stream on silver table requires ignoreChanges\n",
    "        # No watermark required - Stream to staic join is stateless\n",
    "        df_delta = (spark.readStream\n",
    "                         .option(\"startingVersion\", startingVersion)\n",
    "                         .option(\"ignoreChanges\", True)\n",
    "                         #.option(\"withEventTimeOrder\", \"true\")\n",
    "                         #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                         .table(f\"{self.catalog}.{self.db_name}.user_profile\")\n",
    "                         .join(df_user, [\"user_id\"], \"left\")\n",
    "                         .select(\"user_id\", self.age_bins(F.col(\"dob\")),\"gender\", \"city\", \"state\")\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/user_bins\")\n",
    "                                 .queryName(\"user_bins_upsert_stream\")\n",
    "                        )\n",
    "        \n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "\n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "                  \n",
    "            \n",
    "    def upsert_completed_workouts(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "              \n",
    "        #Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "        MERGE INTO {self.catalog}.{self.db_name}.completed_workouts a\n",
    "        USING completed_workouts_delta b\n",
    "        ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"completed_workouts_delta\")\n",
    "    \n",
    "        df_start = (spark.readStream\n",
    "                         .option(\"startingVersion\", startingVersion)\n",
    "                         .option(\"ignoreDeletes\", True)\n",
    "                         #.option(\"withEventTimeOrder\", \"true\")\n",
    "                         #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                         .table(f\"{self.catalog}.{self.db_name}.workouts\")\n",
    "                         .filter(\"action = 'start'\")                         \n",
    "                         .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as start_time\")\n",
    "                         .withWatermark(\"start_time\", \"30 seconds\")\n",
    "                         #.dropDuplicates([\"user_id\", \"workout_id\", \"session_id\", \"start_time\"])\n",
    "                   )\n",
    "        \n",
    "        df_stop = (spark.readStream\n",
    "                         .option(\"startingVersion\", startingVersion)\n",
    "                         .option(\"ignoreDeletes\", True)\n",
    "                         #.option(\"withEventTimeOrder\", \"true\")\n",
    "                         #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                         .table(f\"{self.catalog}.{self.db_name}.workouts\")\n",
    "                         .filter(\"action = 'stop'\")                         \n",
    "                         .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as end_time\")\n",
    "                         .withWatermark(\"end_time\", \"30 seconds\")\n",
    "                         #.dropDuplicates([\"user_id\", \"workout_id\", \"session_id\", \"end_time\"])\n",
    "                   )\n",
    "        \n",
    "        # State cleanup - Define a condition to clean the state\n",
    "        #               - stop must occur within 3 hours of start \n",
    "        #               - stop < start + 3 hours\n",
    "        join_condition = [df_start.user_id == df_stop.user_id, df_start.workout_id==df_stop.workout_id, df_start.session_id==df_stop.session_id, \n",
    "                          df_stop.end_time < df_start.start_time + F.expr('interval 3 hour')]         \n",
    "        \n",
    "        df_delta = (df_start.join(df_stop, join_condition)\n",
    "                            .select(df_start.user_id, df_start.workout_id, df_start.session_id, df_start.start_time, df_stop.end_time)\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"append\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/completed_workouts\")\n",
    "                                 .queryName(\"completed_workouts_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p1\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def upsert_workout_bpm(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "              \n",
    "        #Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "        MERGE INTO {self.catalog}.{self.db_name}.workout_bpm a\n",
    "        USING workout_bpm_delta b\n",
    "        ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id AND a.time=b.time\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        \n",
    "        data_upserter=Upserter(query, \"workout_bpm_delta\")        \n",
    "        \n",
    "        df_users = spark.read.table(\"users\")\n",
    "        \n",
    "        df_completed_workouts = (spark.readStream\n",
    "                                      .option(\"startingVersion\", startingVersion)\n",
    "                                      .option(\"ignoreDeletes\", True)\n",
    "                                      #.option(\"withEventTimeOrder\", \"true\")\n",
    "                                      #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                                      .table(f\"{self.catalog}.{self.db_name}.completed_workouts\")\n",
    "                                      .join(df_users, \"user_id\")\n",
    "                                      .selectExpr(\"user_id\", \"device_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\")\n",
    "                                      .withWatermark(\"end_time\", \"30 seconds\")\n",
    "                                 )\n",
    "        \n",
    "        df_bpm = (spark.readStream\n",
    "                       .option(\"startingVersion\", startingVersion)\n",
    "                       .option(\"ignoreDeletes\", True)\n",
    "                       #.option(\"withEventTimeOrder\", \"true\")\n",
    "                       #.option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                       .table(f\"{self.catalog}.{self.db_name}.heart_rate\")\n",
    "                       .filter(\"valid = True\")                         \n",
    "                       .selectExpr(\"device_id\", \"time\", \"heartrate\")\n",
    "                       .withWatermark(\"time\", \"30 seconds\")\n",
    "                   )\n",
    "        \n",
    "        # State cleanup - Define a condition to clean the state\n",
    "        #               - Workout could be a maximum of three hours\n",
    "        #               - workout must end within 3 hours of bpm \n",
    "        #               - workout.end < bpm.time + 3 hours\n",
    "        join_condition = [df_completed_workouts.device_id == df_bpm.device_id, \n",
    "                          df_bpm.time > df_completed_workouts.start_time, df_bpm.time <= df_completed_workouts.end_time,\n",
    "                          df_completed_workouts.end_time < df_bpm.time + F.expr('interval 3 hour')] \n",
    "        \n",
    "        df_delta = (df_bpm.join(df_completed_workouts, join_condition)\n",
    "                          .select(\"user_id\", \"workout_id\",\"session_id\", \"start_time\", \"end_time\", \"time\", \"heartrate\")\n",
    "                   )\n",
    "        \n",
    "        stream_writer = (df_delta.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsert)\n",
    "                                 .outputMode(\"append\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/workout_bpm\")\n",
    "                                 .queryName(\"workout_bpm_upsert_stream\")\n",
    "                        )\n",
    "\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p2\")\n",
    "        \n",
    "        if once == True:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    def _await_quries(self,once):\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "    \n",
    "    def upsert(self, once=True, processing_time=\"5 seconds\"):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nExecuting silver layer upsert ...\")\n",
    "        self.upsert_users(once, processing_time)\n",
    "        self.upsert_gym_logs(once, processing_time)\n",
    "        self.upsert_user_profile(once, processing_time)\n",
    "        self.upsert_workouts(once, processing_time)\n",
    "        self.upsert_heart_rate(once, processing_time)        \n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 1 upsert {int(time.time()) - start} seconds\")\n",
    "        self.upsert_user_bins(once, processing_time)\n",
    "        self.upsert_completed_workouts(once, processing_time)        \n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 2 upsert {int(time.time()) - start} seconds\")\n",
    "        self.upsert_workout_bpm(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 3 upsert {int(time.time()) - start} seconds\")\n",
    "\n",
    "    def assert_count(self, table_name, expected_count, filter=\"true\"):\n",
    "        print(f\"Validating record counts in {table_name}...\", end='')\n",
    "        actual_count = spark.read.table(f\"{self.catalog}.{self.db_name}.{table_name}\").where(filter).count()\n",
    "        assert actual_count == expected_count, f\"Expected {expected_count:,} records, found {actual_count:,} in {table_name} where {filter}\" \n",
    "        print(f\"Found {actual_count:,} / Expected {expected_count:,} records where {filter}: Success\")   \n",
    "        \n",
    "    def validate(self, sets):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nValidating silver layer records...\")\n",
    "        self.assert_count(\"users\", 5 if sets == 1 else 10)\n",
    "        self.assert_count(\"gym_logs\", 8 if sets == 1 else 16)\n",
    "        self.assert_count(\"user_profile\", 5 if sets == 1 else 10)\n",
    "        self.assert_count(\"workouts\", 16 if sets == 1 else 32)\n",
    "        self.assert_count(\"heart_rate\", sets * 253801)\n",
    "        self.assert_count(\"user_bins\", 5 if sets == 1 else 10)\n",
    "        self.assert_count(\"completed_workouts\", 8 if sets == 1 else 16)\n",
    "        self.assert_count(\"workout_bpm\", 3968 if sets == 1 else 8192)\n",
    "        print(f\"Silver layer validation completed in {int(time.time()) - start} seconds\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-silver-layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
